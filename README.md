Forecasting Inflation: Polynomial Feature Engineering & Regularization
ğŸ“Œ Project Overview
This project investigates a central challenge in econometrics: Can machine learning improve short-term inflation forecasting by capturing non-linear relationships? Using global economic indicators (M2 Money Stock, Oil Prices, and CPI data), I developed a predictive framework that compares traditional Ordinary Least Squares (OLS) against Ridge and Lasso regression models. The core of the research focuses on the impact of Polynomial Feature Engineeringâ€”expanding the feature space to capture complex interactions between economic drivers.

ğŸ› ï¸ Tech Stack
Language: Python 3.x

Libraries: Scikit-Learn, Pandas, NumPy, Matplotlib, Seaborn

Methodology: Time-Series Analysis, K-Fold Cross-Validation, Hyperparameter Tuning (Alpha), L1/L2 Regularization.

ğŸ”¬ Key Research Questions & Findings
The Complexity Paradox: My research tested whether higher-order interactions improve accuracy or simply introduce noise (overfitting).

Regularization is Non-Negotiable: As model complexity increased via polynomial expansion, OLS performance degraded significantly. Lasso (L1) and Ridge (L2) models were utilized to "shrink" coefficients, effectively managing the bias-variance tradeoff.

Feature Selection: Lasso regression was specifically leveraged for its ability to perform automated feature selection, identifying which economic interactions were truly predictive of global inflation trends.

ğŸ“‚ Repository Structure
Inflation_Forecasting.ipynb: Complete Python implementation (Pre-processing â†’ Modeling â†’ Evaluation).



ğŸš€ How to Use
Clone the repo.

Install dependencies: pip install -r requirements.txt.

Run the Notebook: Follow the step-by-step implementation of the polynomial transformations and model comparisons.

ğŸ“– Citation & Academic Context
This project was developed as part of my MSc in Data Science at the University of Exeter. It bridges the gap between traditional economic theory and modern computational statistics.
